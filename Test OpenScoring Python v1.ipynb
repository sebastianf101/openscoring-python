{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de OpenScoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes hay que instalar y levantar el servicio que está implementado en Java.\n",
    "\n",
    "```\n",
    "version=2.1-SNAPSHOT\n",
    "cd openscoring-server/target\n",
    "java -jar openscoring-server-executable-${version}.jar --port 8181\n",
    "\n",
    "```\n",
    "\n",
    "La librería que usa el cliente se instala con:\n",
    "\n",
    "```\n",
    "pip install --upgrade git+https://github.com/sebastianf101/openscoring-python.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openscoring import Openscoring\n",
    "from openscoring import EvaluationRequest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión al servicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os = Openscoring(base_url = \"http://localhost:8181/openscoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de modelo básico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos un árbol de decisión básico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openscoring.common.ModelResponse at 0x229fefc6710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.deployFile(\"Iris\", \"openscoring/tests/resources/DecisionTreeIris.pmml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Species': 'setosa', 'probability(setosa)': 1.0, 'probability(versicolor)': 0.0, 'probability(virginica)': 0.0}\n"
     ]
    }
   ],
   "source": [
    "arguments = {\n",
    "\t\"Sepal.Length\" : 5.1,\n",
    "\t\"Sepal.Width\" : 3.5,\n",
    "\t\"Petal.Length\" : 1.4,\n",
    "\t\"Petal.Width\" : 0.2\n",
    "}\n",
    "\n",
    "results = os.evaluate(\"Iris\", arguments)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Species': 'setosa', 'probability(setosa)': 1.0, 'probability(versicolor)': 0.0, 'probability(virginica)': 0.0}\n"
     ]
    }
   ],
   "source": [
    "evaluationRequest = EvaluationRequest(\"record-001\", arguments)\n",
    "\n",
    "evaluationResponse = os.evaluate(\"Iris\", evaluationRequest)\n",
    "print(evaluationResponse.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación por Lotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos las filas del csv input con el modelo cargado y ponemos el resultado en el csv output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.evaluateCsvFile(\"Iris\", \"openscoring/tests/resources/input.csv\", \"openscoring/tests/resources/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Id     Species  probability(setosa)  probability(versicolor)  \\\n",
      "0  record-001      setosa                  1.0                 0.000000   \n",
      "1  record-002  versicolor                  0.0                 0.907407   \n",
      "2  record-003   virginica                  0.0                 0.021739   \n",
      "\n",
      "   probability(virginica)  \n",
      "0                0.000000  \n",
      "1                0.092593  \n",
      "2                0.978261  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"openscoring/tests/resources/output.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Modelo con transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otro cuaderno (debido al cambio de entorno) intentamos exportar con formato PMML el modelo que se hizo para el Macro con el módulo `sklearn2pmml` que forma parte de la oferta de productos de OpenScoring.   \n",
    "\n",
    "Recortando el pipeline del modelo dejando sólo los pasos que usan sci-kit learn \"puro\" se pudo exportar a un archivo PMML pero luego no se pudo realizar el `deploy`.   \n",
    "\n",
    "Por ejemplo, el siguiente recorte no funcionó.  \n",
    "\n",
    "```\n",
    "model_pipe = PMMLPipeline(steps=[\n",
    "    ('ImputarMedias', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('Discretizar', KBinsDiscretizer(encode=\"onehot\", random_state=0)),\n",
    "    ('LogReg', LogisticRegression(random_state=0))\n",
    "])\n",
    "```\n",
    "\n",
    "Si quitamos el paso 'Discretizar' si funciona el `deploy` y la evaluación. \n",
    "\n",
    "Luego probamos apartarnos más de lo hecho para el Macro y seguir en cambio el método de construcción del Pipeline que figura en la documentación: https://openscoring.io/blog/2020/01/19/converting_logistic_regression_pmml/#scikit-learn .\n",
    "\n",
    "Esto requiere otro módulo, `sklearn_pandas`, para realizar el preprocesamiento.   Probamos realizar la Discretización de esta forma. \n",
    "\n",
    "```\n",
    "# Preproceso\n",
    "from sklearn.preprocessing import KBinsDiscretizer, StandardScaler, OneHotEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn2pmml.decoration import CategoricalDomain, ContinuousDomain\n",
    "from sklearn2pmml.preprocessing import ExpressionTransformer, LookupTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "\t([\"income\", \"Credit_Score\"], [ContinuousDomain(), SimpleImputer(missing_values=np.nan, strategy='mean'), \n",
    "        KBinsDiscretizer(encode=\"onehot\", random_state=101)\n",
    "      ]), \n",
    "\t([\"occupancy_type\"], [CategoricalDomain(), OneHotEncoder(drop = \"first\")])\n",
    "])\n",
    "\n",
    "model_pipe = PMMLPipeline([\n",
    "\t(\"mapper\", mapper),\n",
    "\t(\"classifier\", LogisticRegression(random_state=101))\n",
    "])\n",
    "```\n",
    "\n",
    "y esta vez funcionó! Pudimos exportarlo a formato PMML en el archivo `Modelo_logistico_v1_ALL.pmml.xml`.\n",
    "\n",
    "```\n",
    "X_train_rec = X_train[[\"income\", \"Credit_Score\", \"occupancy_type\"]]\n",
    "model_pipe.fit(X_train_rec, y_train)\n",
    "sklearn2pmml(model_pipe, \"Modelo_logistico_v1_ALL.pmml.xml\")\n",
    "```\n",
    "\n",
    "En Conclusión, **no** se puede exportar cualquier Pipeline,  **tiene** que estar construído de cierta manera.   \n",
    "Dudo que la librería pueda tampoco evaluar cualquier Pipeline en formato PMML. \n",
    "Para eliminar esta duda habría que conseguir ejemplos realistas de pipelines en PMML. \n",
    "\n",
    "Ahora vamos a verificar el 'deployment' y medir los tiempos de evaluación de un lote. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openscoring.common.ModelResponse at 0x2298f3e3a60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.deployFile(\"LogReg\", \"Modelo_logistico_v1_ALL.pmml.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "os.evaluateCsvFile(\"LogReg\", \"X_train_rec.csv\", \"X_train_rec_eval.csv\")\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Status  probability(0)  probability(1)\n",
      "19766       0        0.822634        0.177366\n",
      "65207       0        0.823671        0.176329\n",
      "23564       0        0.631101        0.368899\n",
      "35626       0        0.823671        0.176329\n",
      "93864       0        0.739689        0.260311\n",
      "46091       0        0.631101        0.368899\n",
      "88934       0        0.817338        0.182662\n",
      "21150       0        0.633957        0.366043\n",
      "64679       0        0.633957        0.366043\n",
      "71194       0        0.807500        0.192500\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"X_train_rec_eval.csv\")\n",
    "print(df.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de tiempos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La evaluación por 1000 filas tardó: 0.4654071747973433 segundos\n"
     ]
    }
   ],
   "source": [
    "print(f\"La evaluación por 1000 filas tardó: {1000*(end_time-start_time)/df.shape[0]} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta que el pipeline fue muy liviano, de pocos pasos y variables la evaluación no fué tan rápida.   \n",
    "Habría que probar con casos más realistas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posibles próximos pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Evaluar casos más realistas de PMML.  ¿El Cliente va a incluir transformaciones en el PMML?  ¿Qué tiempos de respuesta se esperan?  ¿Será online, batch o ambos el uso esperado?    \n",
    "\n",
    "2. Costó mucho el establecimiento del entorno adecuado tanto para el cliente como para el servicio. \n",
    "No obstante para producción el autor de la librería recomienda para montar un servicio aislado y con autorización usar las implementaciones Docker ó AWS: \n",
    "\"The default user authorization logic is implemented by the org.openscoring.service.filters.NetworkSecurityContextFilter JAX-RS filter class, which grants “user” role (read-only) to any address and “admin” role (read and write) to local host addresses.  When looking to upgrade to a more production-like setup, then Openscoring-Docker and Openscoring-Elastic-Beanstalk projects provide good starting points.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
